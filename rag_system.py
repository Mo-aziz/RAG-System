# -*- coding: utf-8 -*-
"""RAG System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TV-v0uLPO6FrK5edkXyArq01ewxc0QO1
"""

!pip install sentence-transformers==2.6.1
!pip install chromadb==0.5.3
!pip install markdown
!pip install beautifulsoup4
!pip install requests
!pip install streamlit
!pip install nltk

"""## Download NLTK punkt"""

import nltk
nltk.download('punkt')
nltk.download("punkt_tab")

"""## Clone MkDocs GitHub repo"""

!git clone https://github.com/mkdocs/mkdocs.git

"""## Load and clean markdown files"""

import os, re
from bs4 import BeautifulSoup

ROOT = "mkdocs/docs"

def read_markdown(path):
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        return f.read()

def clean_markdown(md_text):
    # remove code blocks
    md_text = re.sub(r"```.*?```", "", md_text, flags=re.DOTALL)
    # remove HTML tags
    md_text = BeautifulSoup(md_text, "html.parser").get_text()
    # collapse whitespace
    md_text = re.sub(r"\s+", " ", md_text)
    return md_text.strip()

files = []
for root, dirs, fs in os.walk(ROOT):
    for f in fs:
        if f.endswith(".md"):
            files.append(os.path.join(root, f))

len(files)

"""## Sentence-based chunking"""

from nltk.tokenize import sent_tokenize

def chunk_text(text, max_sentences=5, overlap=2):
    sents = sent_tokenize(text)
    chunks = []
    i = 0
    while i < len(sents):
        chunk = sents[i : i + max_sentences]
        chunks.append(" ".join(chunk))
        i += max_sentences - overlap
    return chunks

chunks = []
for f in files:
    cleaned = clean_markdown(read_markdown(f))
    cks = chunk_text(cleaned)
    for idx, c in enumerate(cks):
        chunks.append({"text": c, "source": f, "id": f"{f}-{idx}"})

len(chunks)

# Save RAG chunks for multimodal use
import json

with open("chunks.jsonl", "w", encoding="utf-8") as f:
    for c in chunks:
        json.dump(c, f)
        f.write("\n")

print("chunks.jsonl saved for multimodal system. Total chunks:", len(chunks))

"""## Create Chroma Vector DB and insert embeddings"""

from chromadb import Client
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import os
os.environ["ANONYMIZED_TELEMETRY"] = "false"

model = SentenceTransformer("all-MiniLM-L6-v2")

client = Client(Settings(persist_directory="./chromadb"))
collection = client.get_or_create_collection(name="mkdocs_rag", metadata={"hnsw:space": "cosine"})

texts = [c["text"] for c in chunks]
ids   = [c["id"] for c in chunks]
meta  = [{"source": c["source"]} for c in chunks]

embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)

collection.add(
    ids=ids,
    documents=texts,
    embeddings=embeddings,
    metadatas=meta
)

print("All chunks stored. Chroma persistence is automatic.")

"""## Use a local open-source LLM"""

!pip install transformers accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct")
model_llm = AutoModelForCausalLM.from_pretrained(
    "tiiuae/falcon-7b-instruct",
    device_map="auto",
    torch_dtype=torch.float16
)

"""## Build RAG query function"""

def answer_query_local(query, k=3):
    q_emb = model.encode([query])
    results = collection.query(query_embeddings=q_emb, n_results=k)
    context = "\n\n".join(results["documents"][0])

    prompt = f"CONTEXT:\n{context}\n\nQUESTION: {query}\nANSWER:"

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    output = model_llm.generate(**inputs, max_new_tokens=300)
    return tokenizer.decode(output[0], skip_special_tokens=True)

print(answer_query_local("How does MkDocs build static sites?"))

"""## **Bonus Points**

# Install CLIP and related packages
"""

!pip install transformers Pillow

"""# Initialize CLIP model for image embeddings"""

from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"

clip_model_name = "openai/clip-vit-base-patch32"
clip_processor = CLIPProcessor.from_pretrained(clip_model_name)
clip_model = CLIPModel.from_pretrained(clip_model_name).to(device).eval()

def embed_image(image_path):
    img = Image.open(image_path).convert("RGB")
    inputs = clip_processor(images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        features = clip_model.get_image_features(**inputs)
        features = features / features.norm(p=2, dim=-1, keepdim=True)
    return features.cpu().numpy()[0]

from chromadb.config import Settings
from chromadb import Client

client = Client(Settings(persist_directory="./chromadb"))
multi_collection = client.get_or_create_collection(
    name="mkdocs_multimodal_text",
    metadata={"type": "text"}
)

print("Multimodal Chroma collection ready")

"""# Add text embeddings to the multimodal collection"""

text_embeddings = model.encode([c["text"] for c in chunks], batch_size=32, show_progress_bar=True)

for i, c in enumerate(chunks):
    multi_collection.add(
        ids=[f"text-{i}"],
        documents=[c["text"]],
        embeddings=[text_embeddings[i].tolist()],
        metadatas=[{"source": c["source"], "id": c["id"]}]
    )

"""# Add image embeddings"""

import os
import shutil
import re
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# Step 5a: Setup directories
DOC_ROOT = Path("mkdocs/docs")  # your markdown docs
IMAGE_OUTPUT_DIR = Path("mkdocs_images_extracted")
IMAGE_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Acceptable image extensions
IMG_EXTS = {".png", ".jpg", ".jpeg", ".gif", ".svg", ".webp"}

# Function to extract images from markdown files
def extract_images_from_markdown(doc_root=DOC_ROOT, output_dir=IMAGE_OUTPUT_DIR):
    md_files = list(doc_root.rglob("*.md"))
    print(f"Found {len(md_files)} markdown files for image extraction.")
    img_records = []  # store metadata about images

    for md in md_files:
        text = md.read_text(encoding="utf-8", errors="ignore")
        # regex to find markdown images: ![alt](path "title")
        for m in re.finditer(r'!\[([^\]]*)\]\(([^)]+)\)', text):
            alt = m.group(1).strip()
            link = m.group(2).strip().split()[0].strip()

            # Skip external images (optional: you could download them if desired)
            if link.startswith("http://") or link.startswith("https://"):
                try:
                    r = requests.get(link, timeout=10)
                    if r.status_code == 200:
                        ext = Path(link.split("?")[0]).suffix or ".png"
                        dest_file = output_dir / f"img_{len(img_records)}{ext}"
                        dest_file.write_bytes(r.content)
                        img_records.append({
                            "img_path": str(dest_file),
                            "source_md": str(md),
                            "alt": alt,
                            "orig_link": link
                        })
                except Exception as e:
                    print("Skipping unreachable external image:", link)
                    continue
            else:
                # relative path inside repo
                img_path = (md.parent / link).resolve()
                if img_path.exists() and img_path.suffix.lower() in IMG_EXTS:
                    dest = output_dir / f"{len(img_records)}{img_path.suffix}"
                    shutil.copyfile(img_path, dest)
                    img_records.append({
                        "img_path": str(dest),
                        "source_md": str(md),
                        "alt": alt,
                        "orig_link": str(link)
                    })
                else:
                    # maybe path has leading slash
                    candidate = (DOC_ROOT / link.lstrip("/")).resolve()
                    if candidate.exists() and candidate.suffix.lower() in IMG_EXTS:
                        dest = output_dir / f"{len(img_records)}{candidate.suffix}"
                        shutil.copyfile(candidate, dest)
                        img_records.append({
                            "img_path": str(dest),
                            "source_md": str(md),
                            "alt": alt,
                            "orig_link": str(link)
                        })
                    else:
                        # not found, skip
                        continue

    # Save metadata
    with open("image_index.json", "w", encoding="utf-8") as fh:
        import json
        json.dump(img_records, fh, ensure_ascii=False, indent=2)

    print(f"Extracted {len(img_records)} images to {output_dir}")
    return img_records

# Run image extraction
images = extract_images_from_markdown()

"""# Multimodal system +testing (after running the RAG System in the begining restart the runtime and run Multimodal block because Chroma already has an “ephemeral” instance running in memory, and it conflicts with the new persist_directory settings."""

# Multimodal System
# SentenceTransformer for TEXT + CLIP for IMAGES


!pip install --upgrade huggingface_hub transformers sentence-transformers==2.6.1 chromadb==0.5.3


from chromadb import Client
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import torch
import os
import json
from IPython.display import display


#  Device
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# Load CLIP for images

clip_model_name = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)
clip_processor = CLIPProcessor.from_pretrained(clip_model_name)

def embed_image_clip(path):
    img = Image.open(path).convert("RGB")
    inputs = clip_processor(images=img, return_tensors="pt").to(device)
    with torch.no_grad():
        emb = clip_model.get_image_features(**inputs)
    return emb[0].cpu()

# Create or load Chroma collection

persist_dir = "./chromadb_multimodal"
client = Client(Settings(persist_directory=persist_dir))

# Delete old collection if exists
try:
    client.delete_collection("mkdocs_multimodal")
except:
    pass

multi_collection = client.get_or_create_collection(
    name="mkdocs_multimodal",
    metadata={"type": "multimodal"},
)
print("Multimodal collection ready.")


# Load text chunks from JSONL

chunks_file = "/content/chunks.jsonl"
loaded_texts = []
loaded_ids = []

print("Loading JSONL chunks...")
with open(chunks_file, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        try:
            obj = json.loads(line)
            text = obj.get("text", "").strip()
            if text:
                loaded_texts.append(text)
                loaded_ids.append(f"chunk-{i}")
        except:
            pass

print(f"Loaded {len(loaded_texts)} text chunks.")


# Embed text with SentenceTransformer

print("Loading SentenceTransformer model for text...")
text_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2", device=device)

def embed_texts_st(text_list):
    return text_model.encode(text_list, batch_size=64, show_progress_bar=True).tolist()

print("Embedding text chunks...")
text_embeddings = embed_texts_st(loaded_texts)

# Add text to Chroma
multi_collection.add(
    documents=loaded_texts,
    embeddings=text_embeddings,
    ids=loaded_ids
)
print("Text chunks inserted successfully.")


# Embed images

image_dir = "/content/mkdocs_images_extracted"
image_paths = [
    os.path.join(image_dir, f)
    for f in os.listdir(image_dir)
    if f.lower().endswith((".png", ".jpg", ".jpeg"))
]

print(f"Images found: {len(image_paths)}")

import torch.nn as nn

# Map 512-dim CLIP image embeddings to 768-dim to match text embeddings
proj_layer = nn.Linear(512, 768).to(device)

for i, path in enumerate(image_paths):
    emb_512 = embed_image_clip(path).to(device)       # 512-dim
    emb_768 = proj_layer(emb_512).cpu().tolist()     # map to 768-dim
    multi_collection.add(
        ids=[f"img-{i}"],
        documents=[path],
        embeddings=[emb_768],
        metadatas=[{"source": "image"}]
    )

print("Image embeddings added (projected to 768-dim).")

# Test queries

# Text query
query_text = "How does MkDocs generate websites?"
q_emb_text = text_model.encode([query_text])[0]
res_text = multi_collection.query(
    query_embeddings=[q_emb_text.tolist()],
    n_results=3
)
print("query_text = How does MkDocs generate websites?")
print("\nText Query Results:")
print(res_text["documents"][0])

from IPython.display import display
from PIL import Image

# Query image
query_image_path = image_paths[0]
# Move embedding to device of projection layer
q_emb_img_512 = embed_image_clip(query_image_path).to(proj_layer.weight.device)
q_emb_img_768 = proj_layer(q_emb_img_512).cpu().tolist()  # project to collection dim


res_img = multi_collection.query(
    query_embeddings=[q_emb_img_768],
    n_results=5
)
print("query_text = How does MkDocs generate websites?")
print("\nTop Image Query Results:")

for doc in res_img["documents"][0]:
    if os.path.exists(doc):
        print(f"Showing image: {doc}")
        display(Image.open(doc))
    else:
        print(f"Skipped non-image result: {doc[:80]}{'...' if len(doc)>80 else ''}")

"""# FastApi app"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile fastapi_app.py
# from fastapi import FastAPI, Form
# from fastapi.responses import JSONResponse
# from fastapi.staticfiles import StaticFiles
# import os
# 
# # Assumes these are already loaded in memory:
# # text_model
# # clip_model, clip_processor
# # collection
# # embed_text(text) and embed_image(img) functions
# 
# app = FastAPI(title="Multimodal RAG API")
# 
# # Mount images folder
# IMAGE_DIR = "/content/mkdocs_images_extracted"  # or "mkdocs_images_extracted"
# app.mount("/images", StaticFiles(directory=IMAGE_DIR), name="images")
# 
# @app.get("/")
# def root():
#     return {"message": "Multimodal RAG API is running!"}
# 
# @app.post("/search")
# async def search(query: str = Form(...), n_results: int = Form(5)):
#     if query.strip() == "":
#         return JSONResponse(status_code=400, content={"error": "Query is empty"})
# 
#     # Embed the text query
#     emb = embed_text(query)
# 
#     # Query the multimodal collection
#     results = collection.query(query_embeddings=[emb], n_results=n_results)
# 
#     response = {"query": query, "results": []}
#     docs = results["documents"][0]
#     metas = results["metadatas"][0]
# 
#     for d, m in zip(docs, metas):
#         item = {"source": m.get("source", "unknown")}
# 
#         # Check if it's an image
#         if d.lower().endswith((".png", ".jpg", ".jpeg")) and os.path.exists(d):
#             filename = os.path.basename(d)
#             item["type"] = "image"
#             item["path"] = f"/images/{filename}"  # Serve via FastAPI static route
#         else:
#             item["type"] = "text"
#             item["text"] = d
# 
#         response["results"].append(item)
# 
#     return response
#

!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
!sudo dpkg -i cloudflared-linux-amd64.deb

"""# FastApi using Cloudflared (copy the link into index.html then run the file it will open the web page)"""

import subprocess
import time

# Start FastAPI
subprocess.Popen(["uvicorn", "fastapi_app:app", "--host", "0.0.0.0", "--port", "8000"])

time.sleep(5)

# Open tunnel
!cloudflared tunnel --url http://localhost:8000 --no-autoupdate